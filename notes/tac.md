# 鱼书摘记之四
>参数更新方法一般有：SGD, Momentum, AdaGrad, Adam

>权重的初始值设定：如果想减小权重的值，一开始就将初始值设为较小的值才是正途。  
>sigmoid和tanh适宜使用Xavier初始值  
>relu适宜使用He初始值

>批量规范化Batch Normalization：改善激活值分布缺乏广度的问题（如sigmoid激活值趋向01分布而导致的梯度消失进而使得学习效率下降），加速深度神经网络学习的收敛速度

![alt text](tac/image.png ':size=600')

>正则化Regularization：防止过拟合  
>· 权值衰减  
>· Dropout

神经网络中，除了权重和偏置等参数，`超参数（hyper-parameter）`也经常出现。这里所说的超参数是指，比如各层的`神经元数量`、`batch大小`、参数更新时的`学习率`或`权值衰减`等。如果这些超参数没有设置合适的值，模型的性能就会很差。虽然超参数的取值非常重要，但是在决定超参数的过程中一般会伴随很多的试错。